{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\ntest = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\n\nuseful_features = list(train.iloc[:, 2:55].columns)\n\n\ny = train.sort_values('TransactionDT')['isFraud']\nX = train.sort_values('TransactionDT')[useful_features]\nX_test = test[useful_features]\ndel train, test\n\nX['mail_to']= X.P_emaildomain.map(str)+'-'+X.R_emaildomain.map(str)\nX_test['mail_to']= X_test.P_emaildomain.map(str)+'-'+X_test.R_emaildomain.map(str)\n\n\nX['card_full'] = X.card1.map(str) +'-'+X.card2.map(str)+ '-' +X.card3.map(str)+'-'+X.card5.map(str)\nX_test['card_full'] = X_test.card1.map(str) +'-'+X_test.card2.map(str)+ '-' +X_test.card3.map(str)+'-'+X_test.card5.map(str)\n\nX['Transaction_day_of_week'] = np.floor((X['TransactionDT'] / (3600 * 24) - 1) % 7)\nX_test['Transaction_day_of_week'] = np.floor((X_test['TransactionDT'] / (3600 * 24) - 1) % 7)\nX['Transaction_hour'] = np.floor(X['TransactionDT'] / 3600) % 24\nX_test['Transaction_hour'] = np.floor(X_test['TransactionDT'] / 3600) % 24\n\nX['P_isproton']=(X['P_emaildomain']=='protonmail.com')\nX['R_isproton']=(X['R_emaildomain']=='protonmail.com')\nX_test['P_isproton']=(X_test['P_emaildomain']=='protonmail.com')\nX_test['R_isproton']=(X_test['R_emaildomain']=='protonmail.com')\n\nX['TransactionAmt_decimal'] = ((X['TransactionAmt'] - X['TransactionAmt'].astype(int)) * 1000).astype(int)\nX_test['TransactionAmt_decimal'] = ((X_test['TransactionAmt'] - X_test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\nX.drop(['TransactionDT'],axis=1, inplace=True)\nX_test.drop(['TransactionDT'],axis=1, inplace=True)\n\n\ncategorical_features = [\n    'ProductCD',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2',\n    'P_emaildomain',\n    'R_emaildomain',\n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9','Transaction_day_of_week','Transaction_hour','mail_to','card_full'\n]\n\n\n\n\ncontinuous_features = list(filter(lambda x: x not in categorical_features, X))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ContinuousFeatureConverter:\n    def __init__(self, name, feature, log_transform):\n        self.name = name\n        self.skew = feature.skew()\n        self.log_transform = log_transform\n        \n    def transform(self, feature):\n        if self.skew > 1:\n            feature = self.log_transform(feature)\n        \n        mean = feature.mean()\n        std = feature.std()\n        return (feature - mean)/(std + 1e-6)        \n    \nfrom tqdm.autonotebook import tqdm\n\nfeature_converters = {}\ncontinuous_features_processed = []\ncontinuous_features_processed_test = []\n\nfor f in tqdm(continuous_features):\n    feature = X[f]\n    feature_test = X_test[f]\n    log = lambda x: np.log10(x + 1 - min(0, x.min()))\n    converter = ContinuousFeatureConverter(f, feature, log)\n    feature_converters[f] = converter\n    continuous_features_processed.append(converter.transform(feature))\n    continuous_features_processed_test.append(converter.transform(feature_test))\n    \ncontinuous_train = pd.DataFrame({s.name: s for s in continuous_features_processed}).astype(np.float64)\ncontinuous_test = pd.DataFrame({s.name: s for s in continuous_features_processed_test}).astype(np.float64)\n\n\ncontinuous_train['isna_sum'] = continuous_train.isna().sum(axis=1)\ncontinuous_test['isna_sum'] = continuous_test.isna().sum(axis=1)\n\ncontinuous_train['isna_sum'] = (continuous_train['isna_sum'] - continuous_train['isna_sum'].mean())/continuous_train['isna_sum'].std()\ncontinuous_test['isna_sum'] = (continuous_test['isna_sum'] - continuous_test['isna_sum'].mean())/continuous_test['isna_sum'].std()\n\nisna_columns = []\n\n\nfor column in tqdm(continuous_features):\n    isna = continuous_train[column].isna()\n    if isna.mean() > 0.:\n        continuous_train[column + '_isna'] = isna.astype(int)\n        continuous_test[column + '_isna'] = continuous_test[column].isna().astype(int)\n        isna_columns.append(column)\n        \ncontinuous_train = continuous_train.fillna(0.)\ncontinuous_test = continuous_test.fillna(0.)\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_train.iloc[:,36:44]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"train  = train.iloc[:,1:55]\n\nfraud = train[train.isFraud == 1]\n\nclear = train[train.isFraud == 0 ]\n\nfraud.iloc[:,35:55]"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import matplotlib.pyplot as plt\n\nfraud['mail_to']= fraud.P_emaildomain.map(str)+'-'+fraud.R_emaildomain.map(str)\nclear['mail_to']= clear.P_emaildomain.map(str)+'-'+clear.R_emaildomain.map(str)\none = clear.mail_to.unique()\ntwo = fraud.mail_to.unique()\ns=0\nfor element in two:\n    if element not in one:\n        s=s+1\n        \nprint(s)\nfraud.sort_values(by=['card1'])\nfraud['card_full'] = fraud.card1.map(str) +'-'+fraud.card2.map(str)+ '-' +fraud.card3.map(str)+'-'+fraud.card5.map(str)\nclear['card_full'] = clear.card1.map(str) +'-'+clear.card2.map(str)+ '-' +clear.card3.map(str)+'-'+clear.card5.map(str)\nblyat = clear.card_full.unique()\ncyka  =  fraud.card_full.unique()\nm=0\n\nfor element in cyka:\n    if element not in blyat:\n        m=m+1"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"clear.describe()"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tqdm.autonotebook import tqdm\n#from category_encoders import one_hot\n\n\ndef categorical_encode(df_train, df_test, categorical_features, n_values=50):\n    df_train = df_train[categorical_features].astype(str)\n    df_test = df_test[categorical_features].astype(str)\n    \n    categories = []\n    for column in tqdm(categorical_features):\n        categories.append(list(df_train[column].value_counts().iloc[: n_values - 1].index) + ['Other'])\n        values2use = categories[-1]\n        df_train[column] = df_train[column].apply(lambda x: x if x in values2use else 'Other')\n        df_test[column] = df_test[column].apply(lambda x: x if x in values2use else 'Other')\n        \n    \n    ohe = OneHotEncoder(categories = categories)\n    ohe.fit(pd.concat([df_train, df_test]))\n    df_train = pd.DataFrame(ohe.transform(df_train).toarray()).astype(np.int8)\n    df_test = pd.DataFrame(ohe.transform(df_test).toarray()).astype(np.int8)\n    return df_train, df_test\n\n\ntrain_categorical, test_categorical = categorical_encode(X, X_test, categorical_features)\n\nX = pd.concat([continuous_train, train_categorical], axis=1)\n#del continuous_train, train_categorical\nX_test = pd.concat([continuous_test, test_categorical], axis=1)\n#del continuous_test, test_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.decomposition import PCA\n\npca = PCA()\npcb = PCA()\npca.fit(X)\npcb.fit(X_test)\n\n\nX_pca=pd.DataFrame(pca.transform(X))\nX_test_pca=pd.DataFrame(pcb.transform(X_test))\n\nimport gc\ngc.collect()"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.decomposition import PCA\n\n\n\nclf = LinearDiscriminantAnalysis(solver = 'svd')\n\ntest = pd.DataFrame()\n\ntest['X'] = clf.fit_transform(X.iloc[:,2:36],y).flatten()\n#test['Xrest'] = clc.fit_transform(X.iloc[:,111:220],y).flatten()\n#test['Xrest2'] = clr.fit_transform(X.iloc[:,221:330],y).flatten()\n#test['Xrest3'] = cld.fit_transform(X.iloc[:,331:440],y).flatten()\n#test['Xrest4'] = clq.fit_transform(X.iloc[:,441:541],y).flatten()\n\nX['lda271'] = test.X\n#X.drop([X.iloc[:,2:36].columns],axis=1,inplace=True)\n#X['lda541'] = test.Xrest\n#X['lda520++'] = test.Xrest2\n#X['lda521++'] = test.Xrest3\n#X['lda522++'] = test.Xrest4\n\nX_test['X'] = clf.transform(X_test.iloc[:,2:36]).flatten()\n#X.drop([X_test.iloc[:,2:54].columns],axis=1,inplace=True)\n#X_test['Xrest'] = clc.transform(X_test.iloc[:,111:220]).flatten()\n#X_test['Xrest2'] = clr.transform(X_test.iloc[:,221:330]).flatten()\n#X_test['Xrest3'] = cld.transform(X_test.iloc[:,331:440]).flatten()\n#X_test['Xrest4'] = clq.transform(X_test.iloc[:,441:541]).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import matplotlib\nimport matplotlib.pyplot as plt\ncolors = (\"red\", \"green\")\n\n\n#plt.scatter(X.lda271 ,y)\nplt.scatter(X['lda522++'],X['lda521++'],c=y,cmap=matplotlib.colors.ListedColormap(colors))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#data_train, data_test, label_train, label_test = train_test_split(X.iloc[:,542:547],y, test_size=0.2, random_state=47)\n\n\nmodel = keras.Sequential([\n    keras.layers.Dense(512, activation= 'relu',kernel_initializer=tf.initializers.he_uniform(),input_shape=(546,)),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(256, activation='relu',kernel_initializer=tf.initializers.he_uniform()),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(128, activation= 'relu' ,kernel_initializer=tf.initializers.he_uniform()),\n    keras.layers.Dropout(0.1),\n    keras.layers.Dense(32, activation= 'relu',kernel_initializer=tf.initializers.he_uniform()),\n    keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) ])\n\n\n\nmodel.compile(optimizer=keras.optimizers.Adam(0.002),\n           loss='binary_crossentropy')\n\n\n\nmodel.fit(X_tr,y_tr,epochs = 6,batch_size = 2048 ,verbose = 2,\n         validation_data=(X_val,y_val))\n\npred = model.predict_proba(X_tr)\npred_2 = model.predict_proba(X_val)\n    \n\n\nprint('F1_score:',f1_score(y_tr , np.round(pred)),'------>',f1_score(y_val , np.round(pred_2)))\nprint('Confusion matrix :',confusion_matrix(y_tr,np.round(pred)),'------>\\n',confusion_matrix(y_val,np.round( pred_2)))\nprint('Roc Score' ,roc_auc_score(y_tr, pred),'------>',roc_auc_score(y_val, pred_2))"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport tensorflow as tf \nimport keras\nfrom keras import layers\nfrom keras import losses\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, accuracy_score,roc_auc_score\nimport matplotlib.pyplot as plt\nfrom keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n\n\nmodels = list()\n\nfor i in range (1,21):\n    models.append(\"model\"+str(i))\n\ndef get_model():\n    modelz = keras.Sequential([\n    \n    keras.layers.Dense(256, activation='relu',kernel_initializer=tf.initializers.he_uniform(),input_shape=(642,)),\n    #keras.layers.BatchNormalization(),\n    keras.layers.Dense(64,activation='relu',kernel_initializer=tf.initializers.he_uniform()),\n    keras.layers.Dense(16,activation='relu',kernel_initializer=tf.initializers.he_uniform()),\n    keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) ])\n    modelz.compile(optimizer='Adam',\n              loss='binary_crossentropy' ,metrics = ['accuracy'])\n    \n    return modelz\n    \n\ndef get_name(pos):\n    return models[pos]\n\n\n\nimport gc\n#gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_ind = int(X.shape[0]*0.80)\n\nX_tr = X.iloc[:split_ind]\nX_val = X.iloc[split_ind:]\n\ny_tr = y.iloc[:split_ind]\ny_val = y.iloc[split_ind:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=20)\nfrom sklearn.model_selection import ShuffleSplit\nkf.get_n_splits(X_tr,y_tr)\nn=0\n\nss = ShuffleSplit(n_splits  =  20 ,test_size = 0.6, random_state = 47)\nss.get_n_splits(X_tr,y_tr)\nfor train_index, test_index in kf.split(X_tr,y_tr):\n    \n    train = X_tr.iloc[train_index]\n    labels_traini = y_tr.iloc[train_index]\n    \n    test_y=y_tr.iloc[test_index]\n    test=X_tr.iloc[test_index]\n    \n    models[n]=get_model()\n    models[n].fit(train,labels_traini,epochs = 8 ,batch_size = 2048,verbose = 2)\n    \n    n=n+1\n    print(n,'model training')\n #print(roc_auc_score(labels_traini,models[n].predict_proba(train)))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cyka = pd.DataFrame(models[2].predict_proba(X_tr).astype('float32').flatten())\n\npredictions = pd.DataFrame()\nfor n in range(0,20):\n    print('predicting',n,'th model')\n    predictions[str(n)] = models[n].predict_proba(X_tr).astype('float64').flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_val = pd.DataFrame()\nfor n in range(0,20):\n    print('predicting',n,'th model')\n    predictions_val[str(n)] = models[n].predict_proba(X_val).astype('float64').flatten()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\n\n#tree = tree.DecisionTreeClassifier(criterion= 'entropy', max_depth=10,splitter = 'random',min_samples_split=2,\n                                 # min_samples_leaf=8,max_features = 12,class_weight={0:1,1:7})\n#tree2 = RandomForestClassifier(n_estimators=500,min_samples_split=2, min_samples_leaf=5,\n#                              criterion= 'entropy',max_features=7, max_depth=2,class_weight={0:1,1:4},random_state = 0)\n#xgb = GradientBoostingClassifier(learning_rate= 0.1, subsample=0.7,n_estimators=100,loss='exponential',max_depth = 4)\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqda = QuadraticDiscriminantAnalysis()\n\nlr = LogisticRegression(random_state=47, solver='saga',max_iter = 200,\n                        penalty = 'l2',class_weight={0:1,1:5},multi_class='multinomial',tol = 0.0001)\n\nlr.fit(predictions, y_tr)\n\n\n\npred = pd.DataFrame(lr.predict_proba(predictions))[1]\npred_2 = pd.DataFrame(lr.predict_proba(predictions_val))[1]\n\n#validation_data=(data_test,label_test)\n\nprint('F1_score:',f1_score(y_tr , np.round(pred)),'------>',f1_score(y_val , np.round(pred_2)))\nprint('Confusion matrix :',confusion_matrix(y_tr,np.round(pred)),'------>\\n',confusion_matrix(y_val,np.round( pred_2)))\nprint('Roc Score' ,roc_auc_score(y_tr, pred),'------>',roc_auc_score(y_val, pred_2))"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelx = keras.Sequential([\n    keras.layers.Dense(64, activation= 'relu',kernel_initializer=tf.initializers.he_uniform(),input_shape=(20,)),\n    keras.layers.Dropout(0.3),\n    #keras.layers.Dense(20, activation='relu',kernel_initializer=tf.initializers.he_uniform()),\n    keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) ])\n\n\n\nmodelx.compile(optimizer=keras.optimizers.Adam(0.01),\n              loss='binary_crossentropy')\n\n\n\nmodelx.fit(predictions,y_tr,epochs = 3,batch_size = 2048,class_weight = {0:1,1:1} ,verbose = 2)\n\n\nmodelx.compile(optimizer=keras.optimizers.Adam(0.001),\n              loss='binary_crossentropy')\n\n\nmodelx.fit(predictions,y_tr,epochs = 1,batch_size = 2048,class_weight = {0:1,1:1} ,verbose = 2)\n\n\nmodelx.compile(optimizer=keras.optimizers.Adam(0.0001),\n              loss='binary_crossentropy')\nmodelx.fit(predictions,y_tr,epochs = 1,batch_size = 10000,class_weight = {0:1,1:1} ,verbose = 2)\n\npred = modelx.predict_proba(predictions)\npred_2 = modelx.predict_proba(predictions_val)\n    \n#validation_data=(data_test,label_test)\n\nprint('F1_score:',f1_score(y_tr , np.round(pred)),'------>',f1_score(y_val , np.round(pred_2)))\nprint('Confusion matrix :',confusion_matrix(y_tr,np.round(pred)),'------>\\n',confusion_matrix(y_val,np.round( pred_2)))\nprint('Roc Score' ,roc_auc_score(y_tr, pred),'------>',roc_auc_score(y_val, pred_2))\n\n\n#model.fit(predictions_val,y_val,epochs = 3,batch_size = 512,class_weight = {0:1,1:1} ,verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_test = pd.DataFrame()\nfor n in range(0,20):\n    print('predicting',n,'th model')\n    predictions_test[str(n)] = models[n].predict_proba(X_test).astype('float64').flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n\n\nsub.isFraud = pd.DataFrame(modelx.predict_proba(predictions_test).flatten())\n\n\n\n\nsub.to_csv('sub_bagging.csv',header =True,index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}