{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\n",
    "test = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\n",
    "\n",
    "useful_features = list(train.iloc[:, 2:55].columns)\n",
    "\n",
    "\n",
    "y = train.sort_values('TransactionDT')['isFraud']\n",
    "X = train.sort_values('TransactionDT')[useful_features]\n",
    "X_test = test[useful_features]\n",
    "del train, test\n",
    "\n",
    "X['mail_to']= X.P_emaildomain.map(str)+'-'+X.R_emaildomain.map(str)\n",
    "X_test['mail_to']= X_test.P_emaildomain.map(str)+'-'+X_test.R_emaildomain.map(str)\n",
    "\n",
    "\n",
    "X['card_full'] = X.card1.map(str) +'-'+X.card2.map(str)+ '-' +X.card3.map(str)+'-'+X.card5.map(str)\n",
    "X_test['card_full'] = X_test.card1.map(str) +'-'+X_test.card2.map(str)+ '-' +X_test.card3.map(str)+'-'+X_test.card5.map(str)\n",
    "\n",
    "X['Transaction_day_of_week'] = np.floor((X['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "X_test['Transaction_day_of_week'] = np.floor((X_test['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "X['Transaction_hour'] = np.floor(X['TransactionDT'] / 3600) % 24\n",
    "X_test['Transaction_hour'] = np.floor(X_test['TransactionDT'] / 3600) % 24\n",
    "\n",
    "X['P_isproton']=(X['P_emaildomain']=='protonmail.com')\n",
    "X['R_isproton']=(X['R_emaildomain']=='protonmail.com')\n",
    "X_test['P_isproton']=(X_test['P_emaildomain']=='protonmail.com')\n",
    "X_test['R_isproton']=(X_test['R_emaildomain']=='protonmail.com')\n",
    "\n",
    "X['TransactionAmt_decimal'] = ((X['TransactionAmt'] - X['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "X_test['TransactionAmt_decimal'] = ((X_test['TransactionAmt'] - X_test['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "\n",
    "X.drop(['TransactionDT'],axis=1, inplace=True)\n",
    "X_test.drop(['TransactionDT'],axis=1, inplace=True)\n",
    "\n",
    "\n",
    "categorical_features = [\n",
    "    'ProductCD',\n",
    "    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2',\n",
    "    'P_emaildomain',\n",
    "    'R_emaildomain',\n",
    "    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9','Transaction_day_of_week','Transaction_hour','mail_to','card_full'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "continuous_features = list(filter(lambda x: x not in categorical_features, X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousFeatureConverter:\n",
    "    def __init__(self, name, feature, log_transform):\n",
    "        self.name = name\n",
    "        self.skew = feature.skew()\n",
    "        self.log_transform = log_transform\n",
    "        \n",
    "    def transform(self, feature):\n",
    "        if self.skew > 1:\n",
    "            feature = self.log_transform(feature)\n",
    "        \n",
    "        mean = feature.mean()\n",
    "        std = feature.std()\n",
    "        return (feature - mean)/(std + 1e-6)        \n",
    "    \n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "feature_converters = {}\n",
    "continuous_features_processed = []\n",
    "continuous_features_processed_test = []\n",
    "\n",
    "for f in tqdm(continuous_features):\n",
    "    feature = X[f]\n",
    "    feature_test = X_test[f]\n",
    "    log = lambda x: np.log10(x + 1 - min(0, x.min()))\n",
    "    converter = ContinuousFeatureConverter(f, feature, log)\n",
    "    feature_converters[f] = converter\n",
    "    continuous_features_processed.append(converter.transform(feature))\n",
    "    continuous_features_processed_test.append(converter.transform(feature_test))\n",
    "    \n",
    "continuous_train = pd.DataFrame({s.name: s for s in continuous_features_processed}).astype(np.float64)\n",
    "continuous_test = pd.DataFrame({s.name: s for s in continuous_features_processed_test}).astype(np.float64)\n",
    "\n",
    "\n",
    "continuous_train['isna_sum'] = continuous_train.isna().sum(axis=1)\n",
    "continuous_test['isna_sum'] = continuous_test.isna().sum(axis=1)\n",
    "\n",
    "continuous_train['isna_sum'] = (continuous_train['isna_sum'] - continuous_train['isna_sum'].mean())/continuous_train['isna_sum'].std()\n",
    "continuous_test['isna_sum'] = (continuous_test['isna_sum'] - continuous_test['isna_sum'].mean())/continuous_test['isna_sum'].std()\n",
    "\n",
    "isna_columns = []\n",
    "\n",
    "\n",
    "for column in tqdm(continuous_features):\n",
    "    isna = continuous_train[column].isna()\n",
    "    if isna.mean() > 0.:\n",
    "        continuous_train[column + '_isna'] = isna.astype(int)\n",
    "        continuous_test[column + '_isna'] = continuous_test[column].isna().astype(int)\n",
    "        isna_columns.append(column)\n",
    "        \n",
    "continuous_train = continuous_train.fillna(0.)\n",
    "continuous_test = continuous_test.fillna(0.)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm.autonotebook import tqdm\n",
    "#from category_encoders import one_hot\n",
    "\n",
    "\n",
    "def categorical_encode(df_train, df_test, categorical_features, n_values=50):\n",
    "    df_train = df_train[categorical_features].astype(str)\n",
    "    df_test = df_test[categorical_features].astype(str)\n",
    "    \n",
    "    categories = []\n",
    "    for column in tqdm(categorical_features):\n",
    "        categories.append(list(df_train[column].value_counts().iloc[: n_values - 1].index) + ['Other'])\n",
    "        values2use = categories[-1]\n",
    "        df_train[column] = df_train[column].apply(lambda x: x if x in values2use else 'Other')\n",
    "        df_test[column] = df_test[column].apply(lambda x: x if x in values2use else 'Other')\n",
    "        \n",
    "    \n",
    "    ohe = OneHotEncoder(categories = categories)\n",
    "    ohe.fit(pd.concat([df_train, df_test]))\n",
    "    df_train = pd.DataFrame(ohe.transform(df_train).toarray()).astype(np.int8)\n",
    "    df_test = pd.DataFrame(ohe.transform(df_test).toarray()).astype(np.int8)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "train_categorical, test_categorical = categorical_encode(X, X_test, categorical_features)\n",
    "\n",
    "X = pd.concat([continuous_train, train_categorical], axis=1)\n",
    "#del continuous_train, train_categorical\n",
    "X_test = pd.concat([continuous_test, test_categorical], axis=1)\n",
    "#del continuous_test, test_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver = 'svd')\n",
    "\n",
    "test = pd.DataFrame()\n",
    "\n",
    "test['X'] = clf.fit_transform(X.iloc[:,2:36],y).flatten()\n",
    "\n",
    "X['lda271'] = test.X\n",
    "\n",
    "X_test['X'] = clf.transform(X_test.iloc[:,2:36]).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, accuracy_score,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "models = list()\n",
    "\n",
    "for i in range (1,21):\n",
    "    models.append(\"model\"+str(i))\n",
    "\n",
    "def get_model():\n",
    "    modelz = keras.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(256, activation='relu',kernel_initializer=tf.initializers.he_uniform(),input_shape=(642,)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(64,activation='relu',kernel_initializer=tf.initializers.he_uniform()),\n",
    "    keras.layers.Dense(16,activation='relu',kernel_initializer=tf.initializers.he_uniform()),\n",
    "    keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) ])\n",
    "    modelz.compile(optimizer='Adam',\n",
    "              loss='binary_crossentropy' ,metrics = ['accuracy'])\n",
    "    \n",
    "    return modelz\n",
    "    \n",
    "\n",
    "def get_name(pos):\n",
    "    return models[pos]\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "#gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ind = int(X.shape[0]*0.80)\n",
    "\n",
    "X_tr = X.iloc[:split_ind]\n",
    "X_val = X.iloc[split_ind:]\n",
    "\n",
    "y_tr = y.iloc[:split_ind]\n",
    "y_val = y.iloc[split_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=20)\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "kf.get_n_splits(X_tr,y_tr)\n",
    "n=0\n",
    "\n",
    "ss = ShuffleSplit(n_splits  =  20 ,test_size = 0.6, random_state = 47)\n",
    "ss.get_n_splits(X_tr,y_tr)\n",
    "for train_index, test_index in kf.split(X_tr,y_tr):\n",
    "    \n",
    "    train = X_tr.iloc[train_index]\n",
    "    labels_traini = y_tr.iloc[train_index]\n",
    "    \n",
    "    test_y=y_tr.iloc[test_index]\n",
    "    test=X_tr.iloc[test_index]\n",
    "    \n",
    "    models[n]=get_model()\n",
    "    models[n].fit(train,labels_traini,epochs = 8 ,batch_size = 2048,verbose = 2)\n",
    "    \n",
    "    n=n+1\n",
    "    print(n,'model training')\n",
    " #print(roc_auc_score(labels_traini,models[n].predict_proba(train)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cyka = pd.DataFrame(models[2].predict_proba(X_tr).astype('float32').flatten())\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "for n in range(0,20):\n",
    "    print('predicting',n,'th model')\n",
    "    predictions[str(n)] = models[n].predict_proba(X_tr).astype('float64').flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val = pd.DataFrame()\n",
    "for n in range(0,20):\n",
    "    print('predicting',n,'th model')\n",
    "    predictions_val[str(n)] = models[n].predict_proba(X_val).astype('float64').flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelx = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation= 'relu',kernel_initializer=tf.initializers.he_uniform(),input_shape=(20,)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    #keras.layers.Dense(20, activation='relu',kernel_initializer=tf.initializers.he_uniform()),\n",
    "    keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) ])\n",
    "\n",
    "\n",
    "\n",
    "modelx.compile(optimizer=keras.optimizers.Adam(0.01),\n",
    "              loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "modelx.fit(predictions,y_tr,epochs = 3,batch_size = 2048,class_weight = {0:1,1:1} ,verbose = 2)\n",
    "\n",
    "\n",
    "modelx.compile(optimizer=keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "modelx.fit(predictions,y_tr,epochs = 1,batch_size = 2048,class_weight = {0:1,1:1} ,verbose = 2)\n",
    "\n",
    "\n",
    "modelx.compile(optimizer=keras.optimizers.Adam(0.0001),\n",
    "              loss='binary_crossentropy')\n",
    "modelx.fit(predictions,y_tr,epochs = 1,batch_size = 10000,class_weight = {0:1,1:1} ,verbose = 2)\n",
    "\n",
    "pred = modelx.predict_proba(predictions)\n",
    "pred_2 = modelx.predict_proba(predictions_val)\n",
    "    \n",
    "#validation_data=(data_test,label_test)\n",
    "\n",
    "print('F1_score:',f1_score(y_tr , np.round(pred)),'------>',f1_score(y_val , np.round(pred_2)))\n",
    "print('Confusion matrix :',confusion_matrix(y_tr,np.round(pred)),'------>\\n',confusion_matrix(y_val,np.round( pred_2)))\n",
    "print('Roc Score' ,roc_auc_score(y_tr, pred),'------>',roc_auc_score(y_val, pred_2))\n",
    "\n",
    "\n",
    "#model.fit(predictions_val,y_val,epochs = 3,batch_size = 512,class_weight = {0:1,1:1} ,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = pd.DataFrame()\n",
    "for n in range(0,20):\n",
    "    print('predicting',n,'th model')\n",
    "    predictions_test[str(n)] = models[n].predict_proba(X_test).astype('float64').flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n",
    "\n",
    "\n",
    "sub.isFraud = pd.DataFrame(modelx.predict_proba(predictions_test).flatten())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sub.to_csv('sub_bagging.csv',header =True,index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
